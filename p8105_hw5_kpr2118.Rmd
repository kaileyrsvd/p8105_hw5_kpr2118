---
title: "Homework 5"
author: "Kailey Rishovd"
date: "11/87/2020"
output: github_document
---

```{r message=FALSE}
library(tidyverse)
library(rvest)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6, 
  fig.asp = .6, 
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot.continuous.colour = "viridis", 
  ggplot.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

## Problem 1 

Read in the data. 

#### Data Description: 

The homicide data provided by the Washington Post is information on over 52,000 criminal homicides in 50 large U.S. cities over the past decade. It contains variables that describe the date and location of the killing, if there was an arrest made (or not), basic demographics about the victim (first and last name, race, age, sex). 

```{r message=FALSE}
homicide_df = 
  read_csv("data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"), 
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved", 
      disposition == "Open/No arrest"        ~ "unsolved", 
      disposition == "Closed by arrest"      ~ "solved",
      )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```

Take a closer look.... 

```{r message=FALSE}
aggregate_df = 
  homicide_df %>%
    group_by(city_state) %>% 
    summarize(
      hom_total = n(), 
      hom_unsolved = sum(resolved == "unsolved")
    )
```

Single city prop test...

```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>%  pull(hom_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```

Iterate... 

```{r}
results_df =
  aggregate_df %>% 
    mutate(
      prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)), 
      tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
    ) %>% 
    select(-prop_tests) %>% 
    unnest(tidy_tests) %>% 
    select(city_state, estimate, conf.low, conf.high)
```


Plot estimates and CIs for each city.... organized by proportion of unsolved homicides...

```{r}
results_df %>% 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


## Problem 2 

Import data and tidy

```{r, message = FALSE}
path_df = 
  tibble(
    path = list.files("data/lda_data"), 
  ) %>% 
    mutate(
      path = str_c("data/lda_data/", path), 
      data = map(path, read_csv)) %>%
  unnest(data) %>% 
  pivot_longer(
    week_1:week_8, 
    names_to = "week", 
    values_to = "observation", 
    names_prefix = "week_"
  ) %>%
  mutate(
    week = as.factor(week)
  ) %>% 
  separate(path, c("path", "arm_id"), sep = "lda_data/") %>% 
  separate(arm_id, c("arm", "id"), sep = "_") %>% 
  separate(id, c("id", "csv")) %>% 
  select(id, arm, week, observation) %>% 
  mutate(
    arm = str_replace(arm, c("con"), "control"), 
    arm = str_replace(arm, c("exp"), "experiment")
  )
```

Spaghetti plot showing observations on each subject over time

```{r}
path_df %>% 
  mutate(
    regrouped = str_c(id, arm, sep = "_")
  ) %>% 
  ggplot(aes(x = week, y = observation, group = regrouped, color = arm)) +
  geom_line() +
  labs(title = "Observations per subject over time") +
  xlab("Time (weeks)") + 
  ylab("Observation Value")
```

The plot above shows the observation values for each subject over time, pertaining to the longitudinal study performed with a control and experimental arm. 

From the plot, we see that the control arm and experimental arm start off having quite a bit of overlap in terms of observation values for the first 2-3 weeks. From then onward, the groups start to move distinctly in their own directions with those in the experimental group obtaining higher values over the next several weeks and those in the control group move closer towards a similar value bracket - just under 0.0 and just under 2.5 - (with a smaller observation value than those in the experimental group, overall). By week 7 there is no overlap between the groups and it becomes clear that the experiment is having some sort of impact on observation value -- with those in the experimental arm rising in observation value.  

## Problem 3 

Simulation to explore power in a one-sample t-test.... 

Create function

```{r}
sim_one_samp = function(n = 30, mu = 0, sigma = 5) { 
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  sim_data %>% 
    t.test(mu = 0, conf.level = 0.95) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
  
}
```

Repeat simulation for mu = 1:6...

```{r}
sim_results = 
  tibble(
    mu = c(0, 1, 2, 3, 4, 5, 6)
  ) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun(5000, sim_one_samp(mu = .x))), 
    estimate_df = map(output_lists, bind_rows)
    ) %>% 
    select(-output_lists) %>% 
    unnest(estimate_df) %>% 
  mutate(
    reject = case_when(
      p.value <= 0.05 ~ "reject", 
      p.value > 0.05 ~ "FTR"
    )
  )
```

Let's make some plots from this simulation... 

To look at the proportion of times the null was rejected (power) vs. true value of mu

```{r message=FALSE}
sim_results %>% 
  group_by(mu) %>% 
  summarize(proportion_rej = mean(reject == "reject")) %>% 
  ggplot(aes(x = mu, y = proportion_rej)) +
  geom_point() +
  geom_line() + 
  ylab("Proportion Rejected") +
  xlab("Mu (mean)") +
  labs(title = "Power per mu value")
```

The plot above shows that the null was rejected more often as the mu got larger. This visualization helps us to see that power (the probability that a false null hypothesis is rejected) increases as the effect size increases. Thus, as the mean difference (mu) gets larger, power goes up. 


Now let's look at the average estimate (mu_hat) compared to the true value of mu... 

```{r message=FALSE}
avg_est_p = 
  sim_results %>% 
    group_by(mu) %>% 
    summarize(avg_est = mean(estimate)) %>% 
    ggplot(aes(x = mu, y = avg_est)) + 
    geom_point() + 
    geom_line() + 
    xlab("Mu (mean)") +
    ylab("Average Estimate mu_hat") + 
    labs(title = "Avg estimate vs. true value of mu")

avg_est_rej_p = 
  sim_results %>% 
    group_by(mu) %>% 
    filter(reject == "reject") %>% 
    summarize(avg_est2 = mean(estimate)) %>% 
    ggplot(aes(x = mu, y = avg_est2)) + 
    geom_point() + 
    geom_line() + 
    xlab("Mu (mean)") +
    ylab("Average Estimate mu_hat") + 
    labs(title = "Avg estimate vs. true value of mu (for 'reject')")


avg_est_p + avg_est_rej_p

```

The two plots above show us that the sample average of mu_hat across tests for which the null is rejected becomes more equal to the true value of mu as mu increases. However, when thinking about why this may be, it becomes more clear that the average estimate of mu_hat when the null is rejected is equal to the true value of mu when mu = 0, 4, 5, and 6. The average estimate of mu_hat when the null is rejected does not equal the true value of mu when mu = 1, 2, and 3. And more, the lower the value of mu, the further away is the estimate for the sample average mu_hat when the null is rejected. 

Looking at the proportion of times that the null is rejected per mu value, there is a similarity. It seems that when the null is not being rejected as often (mu = 0) and when the null is being rejected most often (mu = 4, 5, and 6), the sample average of mu_hat across tests for which the null is rejected approximately equals to the true value of mu. 

When mu = 1, 2, and 3, the average estimate when the null is rejected, is far from the true mu (and further away from the truth when the mu value is lower). 

The sample average of mu_hat when the null is rejected, across tests, therefore, does not seem to be a good approximation of the true mean because it is more dependent on the proportion of times the null is rejected than anything else. 

